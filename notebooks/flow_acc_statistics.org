#+title: Flow Acc Statistics
#+PROPERTY: header-args: :python /home/jo/micromamba/envs/torchland/bin/python :session flowaccstats

Again, we calculate statistics for normalization.  We do it here so we don't need to redoit every time, loading data on write limited servers.

#+begin_src python :results output
import sqlite3
import numpy as np
import os
import json

data_path = "../peclet_batch_one_flowacc"
db_path = "../model_runs.db"
split_by = "model_param.seed"
connection = sqlite3.connect(db_path)
cursor = connection.cursor()
cursor.execute(f"SELECT DISTINCT \"{split_by}\" FROM model_run_params")
categories = [r[0] for r in cursor.fetchall()]
train_fraction = 0.8
split = int((len(categories) * train_fraction))
train_categories = categories[:split]
train_filter = f"WHERE \"{split_by}\" IN ({', '.join([str(c) for c in train_categories])})"
cursor.execute(f"SELECT model_run_id FROM model_run_params {train_filter}")
runs = [r[0] for r in cursor.fetchall()]
input_total_sum = np.zeros(2)
input_total_sum_sq = np.zeros(2)
input_total_count = np.zeros(2)
for run in runs:
    data_array = np.load(os.path.join(data_path, f"{run}.npy"))[:,5:-5,5:-5]
    input_total_sum += np.sum(data_array, axis=(1, 2))
    input_total_sum_sq += np.sum(np.square(data_array), axis=(1, 2))
    input_total_count += data_array.shape[1] * data_array.shape[2]
inputs_mean = input_total_sum / input_total_count
variance = (input_total_sum_sq / input_total_count) - np.square(inputs_mean)
handle_variance = np.any([variance < 0, np.isclose(variance, 0)])
variance[handle_variance] = 0.0
inputs_std = np.sqrt(variance)
handle_std = inputs_std < 1e-7
inputs_std[handle_std] = 1.0
labels = []
limit = connection.getlimit(sqlite3.SQLITE_LIMIT_VARIABLE_NUMBER)
for i in range(0, len(runs), limit):
    current_chunk_runs = runs[i: i+limit]
    placeholders = ', '.join(['?']*len(current_chunk_runs))
    cursor.execute(f"SELECT log_peclet FROM model_run_outputs WHERE model_run_id IN ({placeholders})",current_chunk_runs)
    current_labels = [l[0] for l in cursor.fetchall()]
    labels += current_labels
labels = np.array(labels, dtype=np.float64)
labels_mean = np.mean(labels)
labels_std = np.std(labels)
if labels_std < 1e-7:
    labels_std = 1.0
stats = {'inputs_mean': list(inputs_mean),
         'inputs_std': list(inputs_std),
         'labels_mean': labels_mean,
         'labels_std': labels_std}
with open("full_dataset_statistics.json", 'w') as f:
    json.dump(stats, f)
print(f"Inputs Mean: {inputs_mean}")
print(f"Inputs Std: {inputs_std}\n")
print(f"Labels Mean: {labels_mean}")
print(f"Labels Std: {labels_std}")
#+end_src

#+RESULTS:
: {'inputs_mean': array([ 17.64353269, 558.46255973]), 'inputs_std': array([  30.64808717, 2415.60848769]), 'labels_mean': 7.288967318721593, 'labels_std': 1.1873556332682709}
